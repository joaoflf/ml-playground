{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_alexnet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/joaoflf/ml-playground/blob/master/mnist_alexnet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "i3bggDeRzfAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Application of a Alexnet inspired ConvNet to MNIST"
      ]
    },
    {
      "metadata": {
        "id": "DNNwsTODjQgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2a9a10fa-2849-41c8-c742-96faf8afcdce"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "n_iters = 10000\n",
        "batch_size= 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "n_inputs = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "weight_dict = {\n",
        "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
        "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
        "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
        "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),\n",
        "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
        "}\n",
        "bias_dict = {\n",
        "    'bc1': tf.Variable(tf.random_normal([64])),\n",
        "    'bc2': tf.Variable(tf.random_normal([128])),\n",
        "    'bc3': tf.Variable(tf.random_normal([256])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "def conv(name, input_layer, w, b):\n",
        "  return tf.nn.relu(tf.nn.conv2d(input_layer, w, [1, 1, 1, 1], padding='SAME')+ b, name=name)\n",
        "\n",
        "def max_pool(name, input_layer, k, s):\n",
        "  return tf.nn.max_pool(input_layer, [1, k, k, 1], [1, s, s, 1], padding='SAME', name= name)\n",
        "\n",
        "def norm(name, input_layer, l_size=4):\n",
        "  return tf.nn.lrn(input_layer, l_size, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
        "\n",
        "def conv_net(inputs, weights, biases):\n",
        "  # Reshape input picture\n",
        "  inputs = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
        "  \n",
        "  conv1 = conv('conv1', inputs, weights['wc1'], biases['bc1'])\n",
        "  max_pool1 = max_pool('max_pool1', conv1, 2, 2)\n",
        "  norm1 = norm('norm1', max_pool1)\n",
        "  dropout1 = tf.nn.dropout(norm1, keep_prob)\n",
        "  \n",
        "  conv2 = conv('conv2', dropout1, weights['wc2'], biases['bc2'])\n",
        "  max_pool2 = max_pool('max_pool2', conv2, 2, 2)\n",
        "  norm2 = norm('norm2', max_pool2)\n",
        "  dropout2 = tf.nn.dropout(norm2, keep_prob)\n",
        "  \n",
        "  conv3 = conv('conv3', dropout2, weights['wc3'], biases['bc3'])\n",
        "  max_pool3 = max_pool('max_pool3', conv3, 2, 2)\n",
        "  norm3 = norm('norm3', max_pool3)\n",
        "  dropout3 = tf.nn.dropout(norm3, keep_prob, name='dropout3')\n",
        "  \n",
        "  dense_input = tf.reshape(dropout3, [-1, 4096])\n",
        "  fc1 = tf.nn.relu(tf.matmul(dense_input, weights['wd1']) + biases['bd1'], name='fc1')\n",
        "  fc2 = tf.nn.relu(tf.matmul(fc1, weights['wd2']) + biases['bd2'], name='fc2')\n",
        "  out = tf.matmul(fc2, weights['out']) + biases['out']\n",
        "  \n",
        "  return out\n",
        "\n",
        "y_pred = conv_net(X, weight_dict, bias_dict)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels= y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(n_iters):\n",
        "    X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "    sess.run(optimizer, feed_dict={X: X_batch, y: y_batch, keep_prob:0.8})\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "      loss, acc = sess.run([cost, accuracy], feed_dict={X: X_batch, y: y_batch, keep_prob:1.})\n",
        "      print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \"{:.3f}\".format(loss) + \", Training Accuracy= \" + \"{:.3f}\".format(acc))\n",
        "    \n",
        "  print('Done!')\n",
        "  print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels, keep_prob: 1.}))\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Iter 0, Minibatch Loss= 307048.125000, Training Accuracy= 0.17000\n",
            "Iter 1000, Minibatch Loss= 1800.776855, Training Accuracy= 0.93000\n",
            "Iter 2000, Minibatch Loss= 3769.105713, Training Accuracy= 0.92000\n",
            "Iter 3000, Minibatch Loss= 1387.415283, Training Accuracy= 0.95000\n",
            "Iter 4000, Minibatch Loss= 412.688995, Training Accuracy= 0.95000\n",
            "Iter 5000, Minibatch Loss= 1074.801514, Training Accuracy= 0.95000\n",
            "Iter 6000, Minibatch Loss= 311.232727, Training Accuracy= 0.96000\n",
            "Iter 7000, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
            "Iter 8000, Minibatch Loss= 170.173599, Training Accuracy= 0.97000\n",
            "Iter 9000, Minibatch Loss= 101.374847, Training Accuracy= 0.98000\n",
            "Done!\n",
            "Testing Accuracy: 0.9834\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}